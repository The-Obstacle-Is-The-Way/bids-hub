# arc-bids

Upload the **Aphasia Recovery Cohort (ARC)** dataset to HuggingFace Hub.

> **Status**: Phase 0 (Setup & Exploration)
> **Target HF Repo**: `the-obstacle-is-the-way/arc-aphasia-bids`

## What is ARC?

The [Aphasia Recovery Cohort (ARC)](https://openneuro.org/datasets/ds004884) is a BIDS-formatted neuroimaging dataset containing:

- **230 chronic stroke patients** with aphasia
- **902 scanning sessions** (longitudinal)
- **Imaging**: T1w, T2w, FLAIR, diffusion, fMRI, resting-state (NIfTI)
- **Annotations**: Expert-drawn lesion maps
- **Tabular**: Demographics + WAB (Western Aphasia Battery) scores

**Source**: [OpenNeuro ds004884](https://openneuro.org/datasets/ds004884/versions/1.0.1)
**Paper**: [Scientific Data (2024)](https://www.nature.com/articles/s41597-024-03819-7)
**License**: CC0 (Public Domain) - Redistribution OK

## Goal

Convert ARC from BIDS → HuggingFace Dataset that:
1. Works with `datasets.load_dataset()`
2. Uses HF's `Nifti()` feature type for NIfTI loading
3. Enables `push_to_hub()` workflow
4. Renders in HF Hub viewer via NiiVue integration

## Architecture

### Data Flow

```
OpenNeuro ds004884 (BIDS)
        │
        ▼ AWS S3 or OpenNeuro CLI
Local: data/openneuro/ds004884/  (gitignored)
        │
        ▼ Python walks BIDS tree
pandas DataFrame (paths + metadata)
        │
        ▼ build_hf_dataset()
datasets.Dataset with Nifti() features
        │
        ▼ push_to_hub()
HF Hub (via XET storage)
```

### Key Modules

| Module | Purpose |
|--------|---------|
| `src/arc_bids/core.py` | Generic BIDS→HF Dataset conversion |
| `src/arc_bids/arc.py` | ARC-specific builder (to implement) |
| `src/arc_bids/cli.py` | Typer CLI |
| `scripts/download_arc.sh` | Download ARC from OpenNeuro |

## Quickstart

```bash
# Clone
git clone https://github.com/The-Obstacle-Is-The-Way/arc-aphasia-bids.git
cd arc-aphasia-bids

# Install dependencies
uv sync --all-extras

# Run tests
uv run pytest

# See CLI help
uv run arc-bids --help

# Show dataset info
uv run arc-bids info

# Download ARC dataset
./scripts/download_arc.sh

# Build dataset (once implemented):
uv run arc-bids build data/openneuro/ds004884 --dry-run
```

## Downloading ARC

### Option 1: AWS S3 (Recommended - No Auth)

```bash
aws s3 sync --no-sign-request s3://openneuro.org/ds004884 data/openneuro/ds004884
```

### Option 2: Interactive Script

```bash
./scripts/download_arc.sh
# Choose: 1) Full dataset, 2) Subset for testing, 3) OpenNeuro CLI
```

### Option 3: OpenNeuro CLI (Deno)

```bash
# Install Deno + CLI
curl -fsSL https://deno.land/install.sh | sh
deno install -A --global jsr:@openneuro/cli -n openneuro
openneuro login  # requires API key

# Download
openneuro download ds004884 data/openneuro/ds004884
```

## Project Structure

```
arc-aphasia-bids/
├── src/arc_bids/
│   ├── __init__.py      # Package exports
│   ├── core.py          # Generic BIDS→HF logic
│   ├── config.py        # ARC configuration
│   ├── arc.py           # ARC builder (STUB - to implement)
│   └── cli.py           # Typer CLI
├── scripts/
│   └── download_arc.sh  # Download script
├── tests/
│   ├── test_core_nifti.py   # Core functionality tests
│   └── test_cli_skeleton.py # CLI tests
├── data/                # (gitignored) Local data
│   └── openneuro/ds004884/
├── pyproject.toml       # PEP 621 project config
├── mypy.ini             # Strict typing config
└── README.md
```

## Usage

### As a Library

```python
from pathlib import Path
import pandas as pd
from datasets import Features, Nifti, Value

from arc_bids.core import DatasetBuilderConfig, build_hf_dataset

# Create a file table with paths to NIfTI files
file_table = pd.DataFrame({
    "subject_id": ["sub-001", "sub-002"],
    "t1w": ["/path/to/sub-001_T1w.nii.gz", "/path/to/sub-002_T1w.nii.gz"],
    "age": [25.0, 30.0],
})

# Define the HF Features schema
features = Features({
    "subject_id": Value("string"),
    "t1w": Nifti(),
    "age": Value("float32"),
})

# Build the dataset
config = DatasetBuilderConfig(
    bids_root=Path("/path/to/bids"),
    hf_repo_id="the-obstacle-is-the-way/arc-aphasia-bids",
    dry_run=True,
)
ds = build_hf_dataset(config, file_table, features)

# Access NIfTI data
img = ds[0]["t1w"]  # Returns nibabel.Nifti1Image
data = img.get_fdata()  # Convert to numpy array
```

### CLI

```bash
# Show dataset info
uv run arc-bids info

# Build dataset (dry run - won't push to Hub)
uv run arc-bids build data/openneuro/ds004884 --dry-run

# Build and push to Hub
uv run arc-bids build data/openneuro/ds004884 --no-dry-run
```

## Development

### Prerequisites

- Python 3.10+
- [uv](https://docs.astral.sh/uv/) for dependency management

### Setup

```bash
# Install all dependencies (including dev)
uv sync --all-extras

# Install pre-commit hooks
uv run pre-commit install
```

### Quality Checks

```bash
# Run tests
uv run pytest

# Lint code
uv run ruff check .

# Type check (strict everywhere)
uv run mypy src tests
```

## References

- [HuggingFace Datasets - NIfTI](https://huggingface.co/docs/datasets/nifti_dataset)
- [BIDS Specification](https://bids-specification.readthedocs.io/)
- [OpenNeuro](https://openneuro.org/)
- [nibabel Documentation](https://nipy.org/nibabel/)

## License

Apache-2.0

The ARC dataset is released under CC0 (Public Domain) on OpenNeuro.
